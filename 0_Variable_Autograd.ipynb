{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable # torch의 automatic gradient calculate하는 부분에서 variable을 가지고 옴."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 1.8264e-37, 1.4013e-45],\n",
       "        [1.4013e-45, 0.0000e+00,        nan,        nan],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7551e-40]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor=torch.Tensor(3,4)\n",
    "x_tensor #x_tensor.data 사용할 수 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 1.8264e-37, 1.4013e-45],\n",
       "        [1.4013e-45, 0.0000e+00,        nan,        nan],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7551e-40]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable= Variable(x_tensor) #variable로 감싸주면 값은 똑같아 보이지만 사실 variable이라는 class로 감싸져 있는 것임. \n",
    "x_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 1.8264e-37, 1.4013e-45],\n",
       "        [1.4013e-45, 0.0000e+00,        nan,        nan],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7551e-40]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.data -> wrapped tensor\n",
    "x_variable.data #tensor값들이 들어가게 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#.grad ->gradient of the variable\n",
    "print(x_variable.grad) # gradient값도 저장이됨. 아직 연산이 진행되지 않았기 때문에 None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.requires_grad -> whether variable requires gradient. gradient를 계산할 지 말지의 여부가 들어가 있는 것임.\n",
    "print(x_variable.requires_grad)\n",
    "\n",
    "x_variable=Variable(x_tensor,requires_grad=True) #처음에 만들 때 gradient를 계산할 지 말지 정함.\n",
    "x_variable.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imyeonghui/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n",
      "/Users/imyeonghui/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: volatile was removed (Variable.volatile is always False)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, False, False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.volatile -> inference mode with minimal memory usage #medel를 학습하고 테스트할 때 inference 할 때 많이 쓰는 것. \n",
    "#불필요한 연산 제거하고 앞으로 feed forward만 하게 최적화시키는 것\n",
    "#전체 연상 graph에 volatile이 하나라도 있으면 전체가 inference mode로 바뀜\n",
    "#single volatile variable makes whole graph not requiring gradient -> requires_grad도 자동 false로 바뀜.\n",
    "#테스트할 때 값을 빨리빨리 확인해 볼 수 있음.\n",
    "\n",
    "x_variable=Variable(x_tensor,volatile=True)\n",
    "x_variable.grad, x_variable.requires_grad, x_variable.volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " True,\n",
       " True,\n",
       " tensor([[ 0.0000e+00, -2.5244e-29,  6.7841e+27,  2.5250e-29],\n",
       "         [ 4.2039e-45,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  2.7551e-40]],\n",
       "        requires_grad=True),\n",
       " tensor([[ 0.0000e+00, -1.0097e-28,         inf,  1.0100e-28],\n",
       "         [ 1.6816e-44,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.1020e-39]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[3., 3., inf, 3.],\n",
       "         [3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3.]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모든 gradient 계산 여부가 True로 나옴. \n",
    "#create graph\n",
    "\n",
    "x=Variable(torch.FloatTensor(3,4),requires_grad=True)\n",
    "y=x**2+4*x\n",
    "z=2*y+3\n",
    "\n",
    "x.requires_grad, y.requires_grad, z.requires_grad ,x , y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.backward(gradient,retain_graph,create_graph,retain_variables) #backpropagation을 하는 것임. \n",
    "#compute gradient of current variable w.r.t. graph leaves\n",
    "\n",
    "loss=torch.FloatTensor(3,4)#임의의 loss값을 지정\n",
    "z.backward(loss)#위에 만든 graph에\n",
    "\n",
    "print(x.grad)\n",
    "y.grad, z.grad #None으로 뜨는 이유는 위의 식이 모두 x로 표시될 수 있기 때문임. 모든 graph를 지나가지만, 값이 저장되는 것이 leaf noded인 x에만 저장이 됨.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
